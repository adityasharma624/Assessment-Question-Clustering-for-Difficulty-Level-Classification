{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f1fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] got 3758 problems from /api/problems/all/\n",
      "Total problems to process: 3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "problems: 100%|██████████| 3758/3758 [3:04:00<00:00,  2.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. CSV saved to: output/leetcode_full_metadata.csv\n",
      "Per-problem JSON files are in the output/ folder too.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LeetCode metadata fetcher -> CSV export (comprehensive fields).\n",
    "\n",
    "- Tries /api/problems/all/ for list; falls back to GraphQL pagination.\n",
    "- Queries detailed question fields (with similarQuestions requested as scalar),\n",
    "  parses JSON-string fields like stats and similarQuestions.\n",
    "- Best-effort scrapes /problems/<slug>/discuss/ for a short discussions preview.\n",
    "- Writes a CSV \"output/leetcode_full_metadata.csv\" with one row per problem.\n",
    "  Nested fields (tags, stats_parsed, similarQuestions_parsed, discussions_preview)\n",
    "  are JSON-encoded strings inside CSV fields.\n",
    "\n",
    "Requirements:\n",
    "    pip install requests beautifulsoup4 tqdm\n",
    "\n",
    "Notes:\n",
    " - If you get blocked by Cloudflare, supply a logged-in cookie string in COOKIE_STRING\n",
    "   (e.g. \"LEETCODE_SESSION=...; csrftoken=...\") or run via a headless browser.\n",
    " - Be polite with rate limits; you can increase sleeps if needed.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "BASE = \"https://leetcode.com\"\n",
    "GRAPHQL_URL = BASE + \"/graphql\"\n",
    "PROBLEMS_API = BASE + \"/api/problems/all/\"\n",
    "\n",
    "# Optionally paste cookies here to improve access (LEETCODE_SESSION, csrftoken, ...)\n",
    "COOKIE_STRING = \"\"  # e.g. \"LEETCODE_SESSION=xxx; csrftoken=yyy;\"\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; LeetCodeMetaFetcher/1.0)\",\n",
    "    \"Accept\": \"application/json, text/plain, */*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": BASE + \"/problems/\",\n",
    "    \"Origin\": BASE,\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "}\n",
    "\n",
    "# GraphQL queries\n",
    "PROBLEMLIST_QUERY = \"\"\"\n",
    "query problemsetQuestionList($categorySlug: String, $skip: Int, $limit: Int) {\n",
    "  problemsetQuestionList(\n",
    "    categorySlug: $categorySlug\n",
    "    skip: $skip\n",
    "    limit: $limit\n",
    "  ) {\n",
    "    hasMore\n",
    "    total\n",
    "    questions {\n",
    "      acRate\n",
    "      difficulty\n",
    "      freqBar\n",
    "      frontendQuestionId\n",
    "      isFavor\n",
    "      paidOnly\n",
    "      status\n",
    "      title\n",
    "      titleSlug\n",
    "      topicTags {\n",
    "        name\n",
    "        id\n",
    "        slug\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "QUESTION_DETAIL_QUERY = \"\"\"\n",
    "query questionData($titleSlug: String!) {\n",
    "  question(titleSlug: $titleSlug) {\n",
    "    questionId\n",
    "    title\n",
    "    titleSlug\n",
    "    content\n",
    "    difficulty\n",
    "    likes\n",
    "    dislikes\n",
    "    isPaidOnly\n",
    "    sampleTestCase\n",
    "    codeSnippets {\n",
    "      lang\n",
    "      langSlug\n",
    "      code\n",
    "    }\n",
    "    topicTags {\n",
    "      name\n",
    "      slug\n",
    "      translatedName\n",
    "    }\n",
    "    stats\n",
    "    similarQuestions\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def session_with_cookies():\n",
    "    s = requests.Session()\n",
    "    s.headers.update(DEFAULT_HEADERS)\n",
    "    if COOKIE_STRING:\n",
    "        for pair in [c.strip() for c in COOKIE_STRING.split(\";\") if c.strip()]:\n",
    "            if \"=\" in pair:\n",
    "                k, v = pair.split(\"=\", 1)\n",
    "                s.cookies.set(k.strip(), v.strip(), domain=\"leetcode.com\")\n",
    "    else:\n",
    "        # initial GET to pick up public cookies like csrftoken\n",
    "        try:\n",
    "            s.get(BASE + \"/problems/\", timeout=15)\n",
    "        except Exception:\n",
    "            pass\n",
    "    csrftoken = s.cookies.get(\"csrftoken\") or s.cookies.get(\"CSRFToken\") or s.cookies.get(\"csrf_token\")\n",
    "    if csrftoken:\n",
    "        s.headers.update({\"x-csrftoken\": csrftoken})\n",
    "    return s\n",
    "\n",
    "def graphql_post(session, query, variables):\n",
    "    payload = {\"query\": query, \"variables\": variables}\n",
    "    r = session.post(GRAPHQL_URL, json=payload, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"[GraphQL] HTTP {r.status_code} -> {r.text[:800]}\")\n",
    "        r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_problem_list_via_api(session):\n",
    "    try:\n",
    "        r = session.get(PROBLEMS_API, timeout=20)\n",
    "        if r.status_code == 200:\n",
    "            j = r.json()\n",
    "            if \"stat_status_pairs\" in j:\n",
    "                out = []\n",
    "                for p in j[\"stat_status_pairs\"]:\n",
    "                    stat = p.get(\"stat\", {})\n",
    "                    out.append({\n",
    "                        \"title\": stat.get(\"question__title\"),\n",
    "                        \"titleSlug\": stat.get(\"question__title_slug\"),\n",
    "                        \"frontendQuestionId\": stat.get(\"question_id\"),\n",
    "                        \"acRate\": p.get(\"ac_rate\"),\n",
    "                        \"difficulty\": p.get(\"difficulty\"),\n",
    "                        \"paidOnly\": p.get(\"paid_only\"),\n",
    "                        \"isFavor\": p.get(\"is_favor\"),\n",
    "                        \"status\": p.get(\"status\"),\n",
    "                    })\n",
    "                return out\n",
    "            if \"questions\" in j:\n",
    "                return j[\"questions\"]\n",
    "        else:\n",
    "            print(f\"[API] problems/all/ returned HTTP {r.status_code} - body: {r.text[:500]}\")\n",
    "    except Exception as e:\n",
    "        print(\"[API] exception while fetching problems API:\", e)\n",
    "    return None\n",
    "\n",
    "def fetch_all_problem_list_via_graphql(session, batch_size=100):\n",
    "    all_questions = []\n",
    "    skip = 0\n",
    "    while True:\n",
    "        vars_ = {\"categorySlug\": \"\", \"skip\": skip, \"limit\": batch_size}\n",
    "        r = graphql_post(session, PROBLEMLIST_QUERY, vars_)\n",
    "        if \"errors\" in r:\n",
    "            raise RuntimeError(\"GraphQL error fetching list: \" + str(r[\"errors\"]))\n",
    "        payload = r[\"data\"][\"problemsetQuestionList\"]\n",
    "        questions = payload[\"questions\"]\n",
    "        all_questions.extend(questions)\n",
    "        print(f\"  got {len(all_questions)} / {payload.get('total')}\")\n",
    "        if not payload[\"hasMore\"]:\n",
    "            break\n",
    "        skip += batch_size\n",
    "        time.sleep(0.4)\n",
    "    return all_questions\n",
    "\n",
    "def try_parse_json_field(maybe_json):\n",
    "    \"\"\"Try to parse fields that may be JSON strings; return original value otherwise.\"\"\"\n",
    "    if maybe_json is None:\n",
    "        return None\n",
    "    if isinstance(maybe_json, (dict, list)):\n",
    "        return maybe_json\n",
    "    if isinstance(maybe_json, str):\n",
    "        s = maybe_json.strip()\n",
    "        if not s:\n",
    "            return s\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            # best-effort: replace single quotes\n",
    "            try:\n",
    "                return json.loads(s.replace(\"'\", '\"'))\n",
    "            except Exception:\n",
    "                return maybe_json\n",
    "    return maybe_json\n",
    "\n",
    "def fetch_question_detail(session, slug):\n",
    "    vars_ = {\"titleSlug\": slug}\n",
    "    r = graphql_post(session, QUESTION_DETAIL_QUERY, vars_)\n",
    "    if \"errors\" in r:\n",
    "        return {\"_error\": r[\"errors\"]}\n",
    "    q = r[\"data\"][\"question\"]\n",
    "    if q is None:\n",
    "        return {\"_error\": \"no question data returned\"}\n",
    "    # parse JSON-string fields\n",
    "    q[\"stats_parsed\"] = try_parse_json_field(q.get(\"stats\"))\n",
    "    q[\"similarQuestions_parsed\"] = try_parse_json_field(q.get(\"similarQuestions\"))\n",
    "    return q\n",
    "\n",
    "def fetch_discussions_for_problem(session, slug, max_pages=1):\n",
    "    \"\"\"Best-effort scraping of the problem's discuss page(s) to extract basic post metadata.\"\"\"\n",
    "    out = []\n",
    "    base = f\"{BASE}/problems/{slug}/discuss/\"\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = base if page == 1 else base + f\"?page={page}\"\n",
    "        try:\n",
    "            resp = session.get(url, timeout=20)\n",
    "            if resp.status_code != 200:\n",
    "                out.append({\"_error\": f\"status_code={resp.status_code}\", \"page\": page})\n",
    "                break\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            # try a variety of selectors; LeetCode layout may change\n",
    "            posts = soup.select(\".discuss-item, .question-list-item, .discuss-list li\")\n",
    "            if not posts:\n",
    "                # fallback: try to locate links that look like discuss links\n",
    "                candidates = soup.select(\"a[href*='/problems/'][href*='/discuss/']\")\n",
    "                for c in candidates[:10]:\n",
    "                    out.append({\n",
    "                        \"title\": c.get_text(strip=True),\n",
    "                        \"url\": c.get(\"href\"),\n",
    "                        \"author\": None,\n",
    "                        \"votes\": None,\n",
    "                        \"comment_count\": None\n",
    "                    })\n",
    "                break\n",
    "            for p in posts:\n",
    "                title_tag = p.select_one(\"a.title\") or p.select_one(\"a\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else None\n",
    "                url_rel = title_tag[\"href\"] if title_tag and title_tag.has_attr(\"href\") else None\n",
    "                author_tag = p.select_one(\".author\") or p.select_one(\".user-info a\")\n",
    "                author = author_tag.get_text(strip=True) if author_tag else None\n",
    "                vote_tag = p.select_one(\".vote-count\") or p.select_one(\".vote\")\n",
    "                votes = vote_tag.get_text(strip=True) if vote_tag else None\n",
    "                comment_tag = p.select_one(\".reply-count\") or p.select_one(\".comments\")\n",
    "                comment_count = comment_tag.get_text(strip=True) if comment_tag else None\n",
    "                out.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": url_rel,\n",
    "                    \"author\": author,\n",
    "                    \"votes\": votes,\n",
    "                    \"comment_count\": comment_count\n",
    "                })\n",
    "            time.sleep(0.6)\n",
    "        except Exception as e:\n",
    "            out.append({\"_error\": f\"exception: {str(e)}\", \"page\": page})\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def text_preview(html_content, length=500):\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "    # naive strip of HTML tags for preview\n",
    "    text = re.sub(\"<[^<]+?>\", \"\", html_content)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if len(text) <= length:\n",
    "        return text\n",
    "    return text[:length] + \"...\"\n",
    "\n",
    "def ensure_output_dir(path=\"output\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def write_csv(rows, out_path):\n",
    "    # determine header (use a fixed order)\n",
    "    header = [\n",
    "        \"title\", \"slug\", \"frontendQuestionId\", \"acRate\", \"difficulty\",\n",
    "        \"paidOnly_basic\", \"isFavor\", \"status\",\n",
    "        \"likes\", \"dislikes\", \"isPaidOnly_detail\", \"sampleTestCase\",\n",
    "        \"topicTags_json\", \"stats_parsed_json\", \"similarQuestions_parsed_json\",\n",
    "        \"content_preview\", \"num_discussions\", \"first_discussion_title\", \"first_discussion_url\",\n",
    "        \"fetched_at\"\n",
    "    ]\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header, quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        for r in rows:\n",
    "            writer.writerow(r)\n",
    "\n",
    "def main():\n",
    "    out_dir = ensure_output_dir(\"output\")\n",
    "    session = session_with_cookies()\n",
    "\n",
    "    # 1) get problem list\n",
    "    problem_list = get_problem_list_via_api(session)\n",
    "    if problem_list:\n",
    "        print(f\"[OK] got {len(problem_list)} problems from /api/problems/all/\")\n",
    "    else:\n",
    "        print(\"[WARN] falling back to GraphQL pagination for problem list...\")\n",
    "        problem_list = fetch_all_problem_list_via_graphql(session, batch_size=100)\n",
    "\n",
    "    print(f\"Total problems to process: {len(problem_list)}\")\n",
    "    csv_rows = []\n",
    "    start_time = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "    for p in tqdm(problem_list, desc=\"problems\"):\n",
    "        slug = p.get(\"titleSlug\") or p.get(\"title_slug\")\n",
    "        if not slug:\n",
    "            continue\n",
    "        basic = p\n",
    "        try:\n",
    "            detail = fetch_question_detail(session, slug)\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"[ERROR] GraphQL detail failed for {slug}: {e}\")\n",
    "            detail = {\"_error\": f\"HTTPError: {e}\"}\n",
    "        except Exception as e:\n",
    "            detail = {\"_error\": str(e)}\n",
    "\n",
    "        # fetch a small discussions preview (best-effort)\n",
    "        try:\n",
    "            discussions = fetch_discussions_for_problem(session, slug, max_pages=1)\n",
    "        except Exception as e:\n",
    "            discussions = [{\"_error\": str(e)}]\n",
    "\n",
    "        # prepare CSV row\n",
    "        topic_tags = detail.get(\"topicTags\") if detail and isinstance(detail, dict) else basic.get(\"topicTags\")\n",
    "        stats_parsed = detail.get(\"stats_parsed\") if detail and isinstance(detail, dict) else None\n",
    "        similar_parsed = detail.get(\"similarQuestions_parsed\") if detail and isinstance(detail, dict) else None\n",
    "\n",
    "        first_discussion = discussions[0] if discussions else None\n",
    "        content_preview = text_preview(detail.get(\"content\") if detail else None, length=500)\n",
    "\n",
    "        row = {\n",
    "            \"title\": detail.get(\"title\") if detail and isinstance(detail, dict) else basic.get(\"title\"),\n",
    "            \"slug\": slug,\n",
    "            \"frontendQuestionId\": basic.get(\"frontendQuestionId\") or basic.get(\"frontend_question_id\") or None,\n",
    "            \"acRate\": basic.get(\"acRate\") or basic.get(\"ac_rate\") or None,\n",
    "            \"difficulty\": basic.get(\"difficulty\") or detail.get(\"difficulty\") if detail else None,\n",
    "            \"paidOnly_basic\": basic.get(\"paidOnly\") if \"paidOnly\" in basic else basic.get(\"paid_only\"),\n",
    "            \"isFavor\": basic.get(\"isFavor\"),\n",
    "            \"status\": basic.get(\"status\"),\n",
    "            \"likes\": detail.get(\"likes\") if detail else None,\n",
    "            \"dislikes\": detail.get(\"dislikes\") if detail else None,\n",
    "            \"isPaidOnly_detail\": detail.get(\"isPaidOnly\") if detail else None,\n",
    "            \"sampleTestCase\": detail.get(\"sampleTestCase\") if detail else None,\n",
    "            \"topicTags_json\": json.dumps(topic_tags, ensure_ascii=False),\n",
    "            \"stats_parsed_json\": json.dumps(stats_parsed, ensure_ascii=False),\n",
    "            \"similarQuestions_parsed_json\": json.dumps(similar_parsed, ensure_ascii=False),\n",
    "            \"content_preview\": content_preview,\n",
    "            \"num_discussions\": len(discussions),\n",
    "            \"first_discussion_title\": first_discussion.get(\"title\") if first_discussion and isinstance(first_discussion, dict) else None,\n",
    "            \"first_discussion_url\": first_discussion.get(\"url\") if first_discussion and isinstance(first_discussion, dict) else None,\n",
    "            \"fetched_at\": start_time\n",
    "        }\n",
    "        csv_rows.append(row)\n",
    "\n",
    "        # write per-problem JSON for backup (useful if interrupted)\n",
    "        with open(os.path.join(out_dir, f\"{slug}.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump({\"basic\": basic, \"detail\": detail, \"discussions_preview\": discussions, \"fetched_at\": start_time}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # polite sleep\n",
    "        time.sleep(0.35)\n",
    "\n",
    "    csv_path = os.path.join(out_dir, \"leetcode_full_metadata.csv\")\n",
    "    write_csv(csv_rows, csv_path)\n",
    "    print(f\"Done. CSV saved to: {csv_path}\")\n",
    "    print(\"Per-problem JSON files are in the output/ folder too.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6ddcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment-clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
